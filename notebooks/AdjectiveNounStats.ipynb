{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607772ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q -r requirements.txt')\n",
    "from adjective_reading import *\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import plotnine as p9\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "964cc7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "CORPUS_DATA = None\n",
    "def get_corpus_data(path_corpus=PATH_CORPUS):\n",
    "    def without_full_text(d):\n",
    "        return {k: v for k, v in d.items() if k != 'fullText'}\n",
    "\n",
    "    global CORPUS_DATA\n",
    "    if CORPUS_DATA is None:\n",
    "        CORPUS_DATA = pd.DataFrame(\n",
    "            tqdm(\n",
    "                (without_full_text(d) for d in orjsonl.stream(path_corpus)),\n",
    "                total=CORPUS_NUM_SENTS\n",
    "            )\n",
    "        ).set_index('url')\n",
    "    return CORPUS_DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b080437a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 71902/71902 [00:03<00:00, 20749.89it/s]\n",
      "  0%|          | 0/71902 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading word2data from /Users/ryan/Dropbox/Share/data/byu_word_data/worddb.byu.txt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 71902/71902 [03:29<00:00, 343.46it/s]\n"
     ]
    }
   ],
   "source": [
    "df_corpus = get_corpus_data()\n",
    "df_instances = get_instances_data()\n",
    "word2data = get_word2data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a6f6749a",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_df = df_instances.merge(df_corpus, on=\"url\", how=\"left\").fillna(\"\")\n",
    "total_df['year'] = pd.to_numeric(total_df.publicationYear, errors=\"coerce\")\n",
    "total_df['decade'] = total_df.year.astype(int) // 10 * 10\n",
    "total_df['prev_pos'] = total_df.token0.map(lambda x: word2data[x]['pos'] if x in word2data else None).fillna(\"\")\n",
    "total_df['next_pos'] = total_df.token2.map(lambda x: word2data[x]['pos'] if x in word2data else None).fillna(\"\")\n",
    "total_df_adjs = total_df[total_df.prev_pos.str.startswith(\"j\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6abaed24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "decade\n",
       "1990    17542\n",
       "2000    16916\n",
       "1980    15094\n",
       "2010    14643\n",
       "1970     8923\n",
       "1960     4821\n",
       "1950     3941\n",
       "1930     2716\n",
       "1940     2701\n",
       "1920     1977\n",
       "1910     1474\n",
       "1900      801\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_df.decade.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "db438908",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(decade\n",
       " 1920    1000\n",
       " 1930    1000\n",
       " 1940    1000\n",
       " 1950    1000\n",
       " 1960    1000\n",
       " 1970    1000\n",
       " 1980    1000\n",
       " 1990    1000\n",
       " 2000    1000\n",
       " 2010    1000\n",
       " Name: count, dtype: int64,\n",
       " (10000, 65))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deccount = 1000\n",
    "total_df_smpl = pd.concat(\n",
    "    gdf.sample(frac=1).head(deccount)\n",
    "    for g,gdf in total_df.query('decade>=1920').groupby('decade')\n",
    ")\n",
    "total_df_smpl.decade.value_counts(), total_df_smpl.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "77b7ba4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# pos_df = total_df_smpl.groupby('decade').agg(\n",
    "#     n_sents=('sent', 'nunique'),\n",
    "#     n_words=('token0', 'size'),\n",
    "#     n_wordtypes=('token0', 'nunique'),\n",
    "#     n_adjs=('prev_pos', lambda x: (x.str.startswith('j')).sum()),\n",
    "#     n_nouns=('next_pos', lambda x: (x.str.startswith('n')).sum()),\n",
    "#     n_verbs=('next_pos', lambda x: (x.str.startswith('v')).sum()),\n",
    "#     n_adverbs=('next_pos', lambda x: (x.str.startswith('r')).sum()),\n",
    "#     n_other=('next_pos', lambda x: (~(x.str.startswith('j')) & ~(x.str.startswith('n')) & ~(x.str.startswith('v')) & ~(x.str.startswith('r'))).sum()),\n",
    "# )\n",
    "# pos_df['ttr_words'] = pos_df.n_wordtypes / pos_df.n_words * 100\n",
    "# pos_df['perc_adj'] = pos_df.n_adjs / pos_df.n_words * 100\n",
    "# pos_df['perc_noun'] = pos_df.n_nouns / pos_df.n_words * 100\n",
    "# pos_df['perc_verb'] = pos_df.n_verbs / pos_df.n_words * 100\n",
    "# pos_df[\"ratio_adj2noun\"] = pos_df.n_adjs / pos_df.n_nouns\n",
    "# round(pos_df,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d13d8e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Series([], Name: sent, dtype: object)\n"
     ]
    }
   ],
   "source": [
    "pprint(total_df_smpl.query('prev_pos==\"ex\"').sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e276c7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pos_type(pos):\n",
    "    if pos.startswith('j'):\n",
    "        return 'adj'\n",
    "    elif pos.startswith('n'):\n",
    "        return 'noun'\n",
    "    elif pos.startswith('v'):\n",
    "        return 'verb'\n",
    "    elif pos.startswith('r'):\n",
    "        return 'adverb'\n",
    "    elif pos.startswith(\"i\"):\n",
    "        return 'preposition'\n",
    "    elif pos.startswith(\"p\"):\n",
    "        return 'pronoun'\n",
    "    elif pos.startswith(\"c\"):\n",
    "        return 'conjunction'\n",
    "    elif pos.startswith(\"d\"):\n",
    "        return 'determiner'\n",
    "    elif pos.startswith(\"a\"):\n",
    "        return 'article'\n",
    "    elif pos.startswith(\"t\"):\n",
    "        return 'particle'\n",
    "    elif pos.startswith(\"m\"):\n",
    "        return 'number'\n",
    "    elif pos.startswith(\"s\"):\n",
    "        return 'symbol'\n",
    "    elif pos.startswith(\"u\"):\n",
    "        return 'punctuation'\n",
    "    elif pos.startswith(\"x\"):\n",
    "        return 'not'\n",
    "    else:\n",
    "        print(pos)\n",
    "        return 'other'\n",
    "\n",
    "def get_stats_from_smpl(df):\n",
    "    old = []\n",
    "    df = df.copy().fillna('')\n",
    "    df = df.query('prev_pos!=\"\" & prev_pos!=\".\" & prev_pos!=\"ex\"')\n",
    "    df['prev_pos0'] = df.prev_pos.str.slice(0, 1)\n",
    "    df['prev_pos_type'] = df.prev_pos.map(get_pos_type)\n",
    "    \n",
    "    for g, gdf in df.groupby('decade'):\n",
    "        tok_counts = gdf.token0.value_counts()\n",
    "        tok_rel_freq = tok_counts / tok_counts.sum()\n",
    "\n",
    "        # pos_counts = gdf.prev_pos0.value_counts()\n",
    "        # pos_rel_freq = pos_counts / pos_counts.sum()\n",
    "        # print(pos_rel_freq)\n",
    "\n",
    "        pos_type_counts = gdf.prev_pos_type.value_counts()\n",
    "        pos_type_rel_freq = pos_type_counts / pos_type_counts.sum()\n",
    "        # print(pos_type_rel_freq)\n",
    "\n",
    "        \n",
    "        for rank, (pos, rel_freq) in enumerate(pos_type_rel_freq.items()):\n",
    "            pos_gdf = gdf.query('prev_pos_type==@pos')\n",
    "            pos_tok_counts = pos_gdf.token0.value_counts()\n",
    "            tok_counts_str= '\\n'.join(f'{tok}\\t{pos_tok_counts[tok]}' for tok in pos_tok_counts.index[:10])\n",
    "\n",
    "            pos_n_tokens = pos_gdf.token0.size\n",
    "            pos_n_wordtypes = pos_gdf.token0.nunique()\n",
    "            pos_d = {\n",
    "                'decade': g,\n",
    "                'pos': pos,\n",
    "                'count': pos_type_counts[pos],\n",
    "                'freq': rel_freq,\n",
    "                'ttr': pos_n_wordtypes / pos_n_tokens * 100,\n",
    "                'egs': '\\n'.join(pos_gdf.sent),\n",
    "                'eg_tokens': tok_counts_str\n",
    "            }\n",
    "            old.append(pos_d)\n",
    "    return pd.DataFrame(old)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d248329",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>decade</th>\n",
       "      <th>pos</th>\n",
       "      <th>count</th>\n",
       "      <th>freq</th>\n",
       "      <th>ttr</th>\n",
       "      <th>egs</th>\n",
       "      <th>eg_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1920</td>\n",
       "      <td>article</td>\n",
       "      <td>259</td>\n",
       "      <td>0.307236</td>\n",
       "      <td>3.474903</td>\n",
       "      <td>The writer of a medieval \"best seller\" -that i...</td>\n",
       "      <td>the\\t176\\na\\t31\\nhis\\t29\\ntheir\\t11\\nmy\\t5\\nit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1920</td>\n",
       "      <td>adj</td>\n",
       "      <td>237</td>\n",
       "      <td>0.281139</td>\n",
       "      <td>51.476793</td>\n",
       "      <td>This is one of the instances in which MS. A al...</td>\n",
       "      <td>wide\\t16\\noriginal\\t15\\ncorrect\\t10\\ncareful\\t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1920</td>\n",
       "      <td>preposition</td>\n",
       "      <td>139</td>\n",
       "      <td>0.164887</td>\n",
       "      <td>11.510791</td>\n",
       "      <td>The Journals of Paris, Amsterdam, Leipsick, Tr...</td>\n",
       "      <td>of\\t43\\nin\\t30\\nby\\t12\\nafter\\t11\\nworth\\t9\\nf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1920</td>\n",
       "      <td>noun</td>\n",
       "      <td>64</td>\n",
       "      <td>0.075919</td>\n",
       "      <td>42.187500</td>\n",
       "      <td>We find Mrs. Frank indebted to Dr. Christ in s...</td>\n",
       "      <td>variant\\t30\\nmanuscript\\t5\\ntext\\t2\\nimpressio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1920</td>\n",
       "      <td>verb</td>\n",
       "      <td>40</td>\n",
       "      <td>0.047450</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>Professor A. T. Baker has drawn my attention t...</td>\n",
       "      <td>was\\t8\\nbeen\\t6\\naccepted\\t5\\nis\\t4\\nwere\\t2\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>2010</td>\n",
       "      <td>adverb</td>\n",
       "      <td>20</td>\n",
       "      <td>0.023337</td>\n",
       "      <td>90.000000</td>\n",
       "      <td>In conceptualizing surface __reading__, they d...</td>\n",
       "      <td>closely\\t2\\nfurther\\t2\\njust\\t1\\nalways\\t1\\nho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>2010</td>\n",
       "      <td>particle</td>\n",
       "      <td>9</td>\n",
       "      <td>0.010502</td>\n",
       "      <td>11.111111</td>\n",
       "      <td>Attending to such contact points shifts the in...</td>\n",
       "      <td>to\\t9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>2010</td>\n",
       "      <td>number</td>\n",
       "      <td>5</td>\n",
       "      <td>0.005834</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>The first mis__reading__ results from an un- c...</td>\n",
       "      <td>first\\t3\\nthree\\t1\\none\\t1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>2010</td>\n",
       "      <td>pronoun</td>\n",
       "      <td>3</td>\n",
       "      <td>0.003501</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>Another is James E. Berg’s chapter on Wopsle’s...</td>\n",
       "      <td>them\\t1\\nthemselves\\t1\\nourselves\\t1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>2010</td>\n",
       "      <td>not</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001167</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>His comment could also be a sarcastic one, cau...</td>\n",
       "      <td>not\\t1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>115 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     decade          pos  count      freq         ttr  \\\n",
       "0      1920      article    259  0.307236    3.474903   \n",
       "1      1920          adj    237  0.281139   51.476793   \n",
       "2      1920  preposition    139  0.164887   11.510791   \n",
       "3      1920         noun     64  0.075919   42.187500   \n",
       "4      1920         verb     40  0.047450   50.000000   \n",
       "..      ...          ...    ...       ...         ...   \n",
       "110    2010       adverb     20  0.023337   90.000000   \n",
       "111    2010     particle      9  0.010502   11.111111   \n",
       "112    2010       number      5  0.005834   60.000000   \n",
       "113    2010      pronoun      3  0.003501  100.000000   \n",
       "114    2010          not      1  0.001167  100.000000   \n",
       "\n",
       "                                                   egs  \\\n",
       "0    The writer of a medieval \"best seller\" -that i...   \n",
       "1    This is one of the instances in which MS. A al...   \n",
       "2    The Journals of Paris, Amsterdam, Leipsick, Tr...   \n",
       "3    We find Mrs. Frank indebted to Dr. Christ in s...   \n",
       "4    Professor A. T. Baker has drawn my attention t...   \n",
       "..                                                 ...   \n",
       "110  In conceptualizing surface __reading__, they d...   \n",
       "111  Attending to such contact points shifts the in...   \n",
       "112  The first mis__reading__ results from an un- c...   \n",
       "113  Another is James E. Berg’s chapter on Wopsle’s...   \n",
       "114  His comment could also be a sarcastic one, cau...   \n",
       "\n",
       "                                             eg_tokens  \n",
       "0    the\\t176\\na\\t31\\nhis\\t29\\ntheir\\t11\\nmy\\t5\\nit...  \n",
       "1    wide\\t16\\noriginal\\t15\\ncorrect\\t10\\ncareful\\t...  \n",
       "2    of\\t43\\nin\\t30\\nby\\t12\\nafter\\t11\\nworth\\t9\\nf...  \n",
       "3    variant\\t30\\nmanuscript\\t5\\ntext\\t2\\nimpressio...  \n",
       "4    was\\t8\\nbeen\\t6\\naccepted\\t5\\nis\\t4\\nwere\\t2\\n...  \n",
       "..                                                 ...  \n",
       "110  closely\\t2\\nfurther\\t2\\njust\\t1\\nalways\\t1\\nho...  \n",
       "111                                              to\\t9  \n",
       "112                         first\\t3\\nthree\\t1\\none\\t1  \n",
       "113               them\\t1\\nthemselves\\t1\\nourselves\\t1  \n",
       "114                                             not\\t1  \n",
       "\n",
       "[115 rows x 7 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_df = get_stats_from_smpl(total_df_smpl)\n",
    "pos_df.to_excel('pos_df.xlsx', index=False)\n",
    "pos_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e4a8d731",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_df['wordCount2'] = sum(\n",
    "    total_df.sent.map(lambda s: len(s.split()))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "29e14dc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>decade</th>\n",
       "      <th>num_sents</th>\n",
       "      <th>num_words</th>\n",
       "      <th>word_per_sent</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>decade</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1900</th>\n",
       "      <td>1900</td>\n",
       "      <td>757</td>\n",
       "      <td>2332845386</td>\n",
       "      <td>3081698.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1910</th>\n",
       "      <td>1910</td>\n",
       "      <td>1391</td>\n",
       "      <td>4286641918</td>\n",
       "      <td>3081698.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1920</th>\n",
       "      <td>1920</td>\n",
       "      <td>1899</td>\n",
       "      <td>5852144502</td>\n",
       "      <td>3081698.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1930</th>\n",
       "      <td>1930</td>\n",
       "      <td>2607</td>\n",
       "      <td>8033986686</td>\n",
       "      <td>3081698.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1940</th>\n",
       "      <td>1940</td>\n",
       "      <td>2583</td>\n",
       "      <td>7960025934</td>\n",
       "      <td>3081698.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1950</th>\n",
       "      <td>1950</td>\n",
       "      <td>3789</td>\n",
       "      <td>11676553722</td>\n",
       "      <td>3081698.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1960</th>\n",
       "      <td>1960</td>\n",
       "      <td>4643</td>\n",
       "      <td>14308323814</td>\n",
       "      <td>3081698.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1970</th>\n",
       "      <td>1970</td>\n",
       "      <td>8452</td>\n",
       "      <td>26046511496</td>\n",
       "      <td>3081698.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1980</th>\n",
       "      <td>1980</td>\n",
       "      <td>14004</td>\n",
       "      <td>43156098792</td>\n",
       "      <td>3081698.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1990</th>\n",
       "      <td>1990</td>\n",
       "      <td>16386</td>\n",
       "      <td>50496703428</td>\n",
       "      <td>3081698.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000</th>\n",
       "      <td>2000</td>\n",
       "      <td>15703</td>\n",
       "      <td>48391903694</td>\n",
       "      <td>3081698.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010</th>\n",
       "      <td>2010</td>\n",
       "      <td>13281</td>\n",
       "      <td>40928031138</td>\n",
       "      <td>3081698.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        decade  num_sents    num_words  word_per_sent\n",
       "decade                                               \n",
       "1900      1900        757   2332845386      3081698.0\n",
       "1910      1910       1391   4286641918      3081698.0\n",
       "1920      1920       1899   5852144502      3081698.0\n",
       "1930      1930       2607   8033986686      3081698.0\n",
       "1940      1940       2583   7960025934      3081698.0\n",
       "1950      1950       3789  11676553722      3081698.0\n",
       "1960      1960       4643  14308323814      3081698.0\n",
       "1970      1970       8452  26046511496      3081698.0\n",
       "1980      1980      14004  43156098792      3081698.0\n",
       "1990      1990      16386  50496703428      3081698.0\n",
       "2000      2000      15703  48391903694      3081698.0\n",
       "2010      2010      13281  40928031138      3081698.0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_df2 = total_df.drop_duplicates(subset=['url', 'sent'])\n",
    "\n",
    "decade_counts = total_df2.groupby('decade').wordCount2.sum()\n",
    "decade_num_sents = total_df2.groupby('decade').size()\n",
    "\n",
    "decade_df = pd.DataFrame({\n",
    "    'decade': decade_counts.index,\n",
    "    'num_sents': decade_num_sents,\n",
    "    'num_words': decade_counts,\n",
    "})\n",
    "decade_df['word_per_sent'] = decade_df.num_words / decade_df.num_sents\n",
    "decade_df.to_excel('decade_df.xlsx', index=False)\n",
    "decade_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b3d97ac9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'url': 'http://www.jstor.org/stable/461288',\n",
       " 'page_num': np.int64(4),\n",
       " 'sent_num': np.int64(4),\n",
       " 'sent': 'We are all our lifetime __reading__ the copious sense of this first of forms.',\n",
       " 'context0': \"194 Pascal et le d6s6quilibre ment cette circularit6 de l'univers dans des termes d'une hardiesse toute pascalienne: The eye is the first circle; the horizon which it forms is the second; and throughout nature this primary figure is repeated without end.\\nIt is the highest emblem in the cipher of the world.\\nSt. Augustine described the nature of God as a circle whose center was everywhere, and its circumference nowhere.\",\n",
       " 'context1': 'We are all our lifetime reading the copious sense of this first of forms.',\n",
       " 'context2': 'One moral we have already deduced, in considering the circulatory or compensatory character of every human action.\\n(Essays i, \"Circles\") 4 Cette conception d\\'une compensation circu- laire est belle.\\nAurait-elle 6t6 souscrite par Pascal?\\nOn peut en douter.\\nEt nous verrons pourquoi.\\nEn tout cas il semble que pour Pascal l\\'ame qui adhere \\'a Dieu ne tombe pas dans un repos inerte.\\nEmbarquec mais affermie sur les fleuves de Babylone, toute tendue vers la Sainte Sion, elle occupe plut6t le point central d\\'un mouvement eternel: elle se \"repose\" au sein du mouvement, Ce mouvement de charite est-il une compensa- tion?',\n",
       " 'token_num': np.int64(5),\n",
       " 'token0': 'lifetime',\n",
       " 'token1': 'reading',\n",
       " 'token2': 'the',\n",
       " 'word': 'reading',\n",
       " 'US_or_UK': '  ',\n",
       " 'fpm_BNC': np.float64(64.69),\n",
       " 'fpm_COCA': np.float64(81.73),\n",
       " 'fpm_COHA_1800s': np.float64(51.11),\n",
       " 'fpm_COHA_1900-49': np.float64(53.16),\n",
       " 'fpm_COHA_1950-89': np.float64(55.23),\n",
       " 'fpm_SOAP': np.float64(13.84),\n",
       " 'fpm_bnc_acad': np.float64(114.08),\n",
       " 'fpm_bnc_fic': np.float64(39.73),\n",
       " 'fpm_bnc_mag': np.float64(48.2),\n",
       " 'fpm_bnc_misc': np.float64(70.75),\n",
       " 'fpm_bnc_news': np.float64(29.71),\n",
       " 'fpm_bnc_noAc': np.float64(93.85),\n",
       " 'fpm_bnc_spok': np.float64(40.65),\n",
       " 'fpm_coca_acad': np.float64(212.98),\n",
       " 'fpm_coca_fic': np.float64(59.14),\n",
       " 'fpm_coca_mag': np.float64(58.28),\n",
       " 'fpm_coca_news': np.float64(50.52),\n",
       " 'fpm_coca_spok': np.float64(31.47),\n",
       " 'freq_COCA': np.int64(37950),\n",
       " 'lemma': 'reading',\n",
       " 'null': '@',\n",
       " 'perc_caps': np.float64(0.1),\n",
       " 'pos': 'nn1',\n",
       " 'rank': np.int64(1135),\n",
       " 'creator': ['Jean-Jacques Demorest'],\n",
       " 'datePublished': '1967-05-01',\n",
       " 'docSubType': 'research-article',\n",
       " 'docType': 'article',\n",
       " 'id': 'http://www.jstor.org/stable/461288',\n",
       " 'identifier': [{'name': 'issn', 'value': '00308129'},\n",
       "  {'name': 'local_uuid', 'value': 'bf1701af-c700-36b5-ad4e-8d30c252f7a1'},\n",
       "  {'name': 'local_doi', 'value': '10.2307/461288'},\n",
       "  {'name': 'journal_id', 'value': 'pmla'}],\n",
       " 'isPartOf': 'PMLA',\n",
       " 'issueNumber': '2',\n",
       " 'language': ['fre'],\n",
       " 'outputFormat': ['unigram', 'bigram', 'trigram'],\n",
       " 'pageCount': np.int64(6),\n",
       " 'pageEnd': '196',\n",
       " 'pageStart': '191',\n",
       " 'pagination': 'pp. 191-196',\n",
       " 'provider': 'jstor',\n",
       " 'publicationYear': np.int64(1967),\n",
       " 'publisher': 'Modern Language Association',\n",
       " 'sourceCategory': ['Humanities', 'Language & Literature'],\n",
       " 'title': 'Pascal et le déséquilibre',\n",
       " 'volumeNumber': '82',\n",
       " 'wordCount': np.int64(4999),\n",
       " 'tdmCategory': '',\n",
       " 'abstract': '',\n",
       " 'subTitle': '',\n",
       " 'year': np.int64(1967),\n",
       " 'decade': np.int64(1960),\n",
       " 'prev_pos': 'nn1',\n",
       " 'next_pos': 'at',\n",
       " 'wordCount2': np.int64(3081698)}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(total_df.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "677d6775",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_corpus_sents(force=True, **kwargs):\n",
    "    df_corpus = get_corpus_data()\n",
    "    return df_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09fee1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q hashstash[rec]\n",
    "from hashstash import HashStash\n",
    "STASH_SENT_COUNTS = HashStash(\"adjread_sent_counts\")\n",
    "# STASH_SENT_COUNTS.clear()\n",
    "# Adjective-noun \"reading\" collocations of interest (for frequency analysis etc.)\n",
    "SENT_COUNT_WORDS = [\n",
    "    \"reading\",\n",
    "    \"distant reading\",      # 2010s = 4x of 2000s\n",
    "    \"correct reading\",      # 1920s = 31x of 2010s\n",
    "    \"careful reading\",      # 1960s = 4x of 2010s\n",
    "    \"political reading\",    # 1980s = 3x of 2010s\n",
    "    \"feminist reading\",     # 1990s = 5x of 2010s\n",
    "    \"nuanced reading\",      # 2010s = 4x of 1990s\n",
    "    \"close reading\",        # 2010s = 33x of 1920s\n",
    "]\n",
    "\n",
    "def get_sent_counts(txt,id=None,force=False):\n",
    "    key=id\n",
    "    if id is not None and not force and key in STASH_SENT_COUNTS:\n",
    "        return STASH_SENT_COUNTS[key]\n",
    "    else:\n",
    "        sents = nltk.sent_tokenize(txt)\n",
    "        tokens = nltk.word_tokenize(txt.lower())\n",
    "        words = txt.split()\n",
    "\n",
    "        \n",
    "        out_ld = []\n",
    "        for keyword in SENT_COUNT_WORDS:\n",
    "            keyword_l = keyword.lower()\n",
    "            num_words_keyword = txt.lower().count(keyword_l) if len(keyword.split())>1 else tokens.count(keyword_l)\n",
    "            num_sents_keyword = len([s for s in sents if keyword_l in s.lower()])\n",
    "\n",
    "            out_d = {\n",
    "                'url': id,\n",
    "                'keyword': keyword,\n",
    "                'keyword_num_words': num_words_keyword,\n",
    "                'keyword_num_sents': num_sents_keyword,\n",
    "                'url_num_words': len(words),\n",
    "                'url_num_sents': len(sents),\n",
    "            }\n",
    "            out_ld.append(out_d)\n",
    "        STASH_SENT_COUNTS[key] = out_ld\n",
    "        return out_ld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "abde6dba",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'orjsonl' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m txt_d \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[43morjsonl\u001b[49m\u001b[38;5;241m.\u001b[39mstream(PATH_CORPUS))\n\u001b[1;32m      2\u001b[0m fulltext \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(txt_d[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfullText\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m      3\u001b[0m get_sent_counts(fulltext,txt_d[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124murl\u001b[39m\u001b[38;5;124m'\u001b[39m], force\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'orjsonl' is not defined"
     ]
    }
   ],
   "source": [
    "txt_d = next(orjsonl.stream(PATH_CORPUS))\n",
    "fulltext = '\\n\\n'.join(txt_d['fullText']).strip()\n",
    "get_sent_counts(fulltext,txt_d['url'], force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "184a7e50",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tqdm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m ld_sents_counts \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m txt_d \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtqdm\u001b[49m(orjsonl\u001b[38;5;241m.\u001b[39mstream(PATH_CORPUS), total\u001b[38;5;241m=\u001b[39mCORPUS_NUM_SENTS):\n\u001b[1;32m      3\u001b[0m     fulltext \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(txt_d[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfullText\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m      4\u001b[0m     ld_sents_counts\u001b[38;5;241m.\u001b[39mextend(get_sent_counts(fulltext,txt_d[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124murl\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tqdm' is not defined"
     ]
    }
   ],
   "source": [
    "ld_sents_counts = []\n",
    "for txt_d in tqdm(orjsonl.stream(PATH_CORPUS), total=CORPUS_NUM_SENTS):\n",
    "    fulltext = '\\n\\n'.join(txt_d['fullText']).strip()\n",
    "    ld_sents_counts.extend(get_sent_counts(fulltext,txt_d['url']))\n",
    "df_sent_counts = pd.DataFrame(ld_sents_counts)\n",
    "df_sent_counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "916dabf0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_sent_counts' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df_sents_corpus \u001b[38;5;241m=\u001b[39m \u001b[43mdf_sent_counts\u001b[49m\u001b[38;5;241m.\u001b[39mmerge(df_corpus[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpublicationYear\u001b[39m\u001b[38;5;124m'\u001b[39m]], on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124murl\u001b[39m\u001b[38;5;124m'\u001b[39m, how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      2\u001b[0m df_sents_corpus[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124myear\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_numeric(df_sents_corpus\u001b[38;5;241m.\u001b[39mpublicationYear, errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcoerce\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m df_sents_corpus[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdecade\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df_sents_corpus\u001b[38;5;241m.\u001b[39myear\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m10\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_sent_counts' is not defined"
     ]
    }
   ],
   "source": [
    "df_sents_corpus = df_sent_counts.merge(df_corpus[['publicationYear']], on='url', how='left')\n",
    "df_sents_corpus['year'] = pd.to_numeric(df_sents_corpus.publicationYear, errors=\"coerce\")\n",
    "df_sents_corpus['decade'] = df_sents_corpus.year.astype(int) // 10 * 10\n",
    "for col in df_sents_corpus.columns:\n",
    "    if col.startswith('num_words_'):\n",
    "        df_sents_corpus[f'perc_{col}'] = df_sents_corpus[col] / df_sents_corpus['num_words'] * 100\n",
    "    if col.startswith('num_sents_'):\n",
    "        df_sents_corpus[f'perc_{col}'] = df_sents_corpus[col] / df_sents_corpus['num_sents'] * 100\n",
    "df_sents_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "1085da40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>decade</th>\n",
       "      <th>num_sents</th>\n",
       "      <th>num_words</th>\n",
       "      <th>num_words_distant_reading</th>\n",
       "      <th>num_sents_distant_reading</th>\n",
       "      <th>num_words_correct_reading</th>\n",
       "      <th>num_sents_correct_reading</th>\n",
       "      <th>num_words_careful_reading</th>\n",
       "      <th>num_sents_careful_reading</th>\n",
       "      <th>num_words_political_reading</th>\n",
       "      <th>num_sents_political_reading</th>\n",
       "      <th>num_words_feminist_reading</th>\n",
       "      <th>num_sents_feminist_reading</th>\n",
       "      <th>num_words_nuanced_reading</th>\n",
       "      <th>num_sents_nuanced_reading</th>\n",
       "      <th>num_words_close_reading</th>\n",
       "      <th>num_sents_close_reading</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1960</td>\n",
       "      <td>1887689</td>\n",
       "      <td>26374462</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>68</td>\n",
       "      <td>67</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>96</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1970</td>\n",
       "      <td>1397012</td>\n",
       "      <td>32729554</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>43</td>\n",
       "      <td>43</td>\n",
       "      <td>72</td>\n",
       "      <td>72</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>298</td>\n",
       "      <td>296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2000</td>\n",
       "      <td>1776111</td>\n",
       "      <td>42284159</td>\n",
       "      <td>46</td>\n",
       "      <td>44</td>\n",
       "      <td>29</td>\n",
       "      <td>28</td>\n",
       "      <td>101</td>\n",
       "      <td>101</td>\n",
       "      <td>54</td>\n",
       "      <td>54</td>\n",
       "      <td>95</td>\n",
       "      <td>95</td>\n",
       "      <td>56</td>\n",
       "      <td>56</td>\n",
       "      <td>1178</td>\n",
       "      <td>1162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1950</td>\n",
       "      <td>1376495</td>\n",
       "      <td>19848818</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>44</td>\n",
       "      <td>44</td>\n",
       "      <td>43</td>\n",
       "      <td>43</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>45</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1980</td>\n",
       "      <td>1572322</td>\n",
       "      <td>36218439</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>48</td>\n",
       "      <td>48</td>\n",
       "      <td>106</td>\n",
       "      <td>106</td>\n",
       "      <td>60</td>\n",
       "      <td>60</td>\n",
       "      <td>120</td>\n",
       "      <td>118</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>587</td>\n",
       "      <td>571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2010</td>\n",
       "      <td>1185118</td>\n",
       "      <td>26842196</td>\n",
       "      <td>135</td>\n",
       "      <td>128</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>62</td>\n",
       "      <td>62</td>\n",
       "      <td>45</td>\n",
       "      <td>45</td>\n",
       "      <td>39</td>\n",
       "      <td>38</td>\n",
       "      <td>61</td>\n",
       "      <td>61</td>\n",
       "      <td>1432</td>\n",
       "      <td>1372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1990</td>\n",
       "      <td>1789294</td>\n",
       "      <td>41221785</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "      <td>29</td>\n",
       "      <td>123</td>\n",
       "      <td>123</td>\n",
       "      <td>71</td>\n",
       "      <td>71</td>\n",
       "      <td>212</td>\n",
       "      <td>211</td>\n",
       "      <td>31</td>\n",
       "      <td>31</td>\n",
       "      <td>875</td>\n",
       "      <td>872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1940</td>\n",
       "      <td>986219</td>\n",
       "      <td>15654020</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>58</td>\n",
       "      <td>55</td>\n",
       "      <td>36</td>\n",
       "      <td>36</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1920</td>\n",
       "      <td>573505</td>\n",
       "      <td>10082313</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>55</td>\n",
       "      <td>55</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1900</td>\n",
       "      <td>312724</td>\n",
       "      <td>4979984</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1910</td>\n",
       "      <td>462222</td>\n",
       "      <td>7878401</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>55</td>\n",
       "      <td>54</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1930</td>\n",
       "      <td>966339</td>\n",
       "      <td>15338574</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>76</td>\n",
       "      <td>71</td>\n",
       "      <td>44</td>\n",
       "      <td>44</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    decade  num_sents  num_words  num_words_distant_reading  \\\n",
       "0     1960    1887689   26374462                          0   \n",
       "1     1970    1397012   32729554                          0   \n",
       "2     2000    1776111   42284159                         46   \n",
       "3     1950    1376495   19848818                          0   \n",
       "4     1980    1572322   36218439                          0   \n",
       "5     2010    1185118   26842196                        135   \n",
       "6     1990    1789294   41221785                          0   \n",
       "7     1940     986219   15654020                          0   \n",
       "8     1920     573505   10082313                          0   \n",
       "9     1900     312724    4979984                          0   \n",
       "10    1910     462222    7878401                          0   \n",
       "11    1930     966339   15338574                          0   \n",
       "\n",
       "    num_sents_distant_reading  num_words_correct_reading  \\\n",
       "0                           0                         38   \n",
       "1                           0                         43   \n",
       "2                          44                         29   \n",
       "3                           0                         44   \n",
       "4                           0                         48   \n",
       "5                         128                          8   \n",
       "6                           0                         29   \n",
       "7                           0                         58   \n",
       "8                           0                         55   \n",
       "9                           0                         20   \n",
       "10                          0                         55   \n",
       "11                          0                         76   \n",
       "\n",
       "    num_sents_correct_reading  num_words_careful_reading  \\\n",
       "0                          38                         68   \n",
       "1                          43                         72   \n",
       "2                          28                        101   \n",
       "3                          44                         43   \n",
       "4                          48                        106   \n",
       "5                           8                         62   \n",
       "6                          29                        123   \n",
       "7                          55                         36   \n",
       "8                          55                         19   \n",
       "9                          20                          7   \n",
       "10                         54                         20   \n",
       "11                         71                         44   \n",
       "\n",
       "    num_sents_careful_reading  num_words_political_reading  \\\n",
       "0                          67                            0   \n",
       "1                          72                            1   \n",
       "2                         101                           54   \n",
       "3                          43                            0   \n",
       "4                         106                           60   \n",
       "5                          62                           45   \n",
       "6                         123                           71   \n",
       "7                          36                            1   \n",
       "8                          19                            0   \n",
       "9                           7                            0   \n",
       "10                         20                            0   \n",
       "11                         44                            0   \n",
       "\n",
       "    num_sents_political_reading  num_words_feminist_reading  \\\n",
       "0                             0                           0   \n",
       "1                             1                           3   \n",
       "2                            54                          95   \n",
       "3                             0                           0   \n",
       "4                            60                         120   \n",
       "5                            45                          39   \n",
       "6                            71                         212   \n",
       "7                             1                           0   \n",
       "8                             0                           0   \n",
       "9                             0                           0   \n",
       "10                            0                           0   \n",
       "11                            0                           0   \n",
       "\n",
       "    num_sents_feminist_reading  num_words_nuanced_reading  \\\n",
       "0                            0                          0   \n",
       "1                            3                          0   \n",
       "2                           95                         56   \n",
       "3                            0                          0   \n",
       "4                          118                          3   \n",
       "5                           38                         61   \n",
       "6                          211                         31   \n",
       "7                            0                          0   \n",
       "8                            0                          0   \n",
       "9                            0                          0   \n",
       "10                           0                          0   \n",
       "11                           0                          0   \n",
       "\n",
       "    num_sents_nuanced_reading  num_words_close_reading  \\\n",
       "0                           0                       96   \n",
       "1                           0                      298   \n",
       "2                          56                     1178   \n",
       "3                           0                       45   \n",
       "4                           3                      587   \n",
       "5                          61                     1432   \n",
       "6                          31                      875   \n",
       "7                           0                       17   \n",
       "8                           0                        3   \n",
       "9                           0                        3   \n",
       "10                          0                        3   \n",
       "11                          0                        8   \n",
       "\n",
       "    num_sents_close_reading  \n",
       "0                        96  \n",
       "1                       296  \n",
       "2                      1162  \n",
       "3                        45  \n",
       "4                       571  \n",
       "5                      1372  \n",
       "6                       872  \n",
       "7                        17  \n",
       "8                         3  \n",
       "9                         3  \n",
       "10                        3  \n",
       "11                        8  "
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_sents_corpus_stats(df):\n",
    "    df=df.copy()\n",
    "    decades = df.decade.unique()\n",
    "    out_ld = []\n",
    "    for decade in decades:\n",
    "        df_decade = df.query('decade==@decade').copy()\n",
    "\n",
    "        decade_sum_words = df_decade.num_words.sum()\n",
    "        decade_sum_sents = len(df_decade)\n",
    "        out_d = {}\n",
    "        out_d['decade'] = decade\n",
    "        for col in df_decade.columns:\n",
    "            if col.startswith('num_'):\n",
    "                summ = df_decade[col].sum()\n",
    "                out_d[f'{col}'] = summ\n",
    "        out_ld.append(out_d)\n",
    "    return pd.DataFrame(out_ld)\n",
    "\n",
    "# df_sents_corpus = get_sents_corpus_stats(df_sents_corpus)\n",
    "\n",
    "# df_sents_corpus['sum_words']\n",
    "# sents_decade_num_words = df_sents_corpus.groupby('decade').num_words.sum()\n",
    "\n",
    "# sents_decade_num_sents = df_sents_corpus.groupby('decade').size()\n",
    "\n",
    "# sents_decade_df = pd.DataFrame({\n",
    "#     'num_sents': sents_decade_num_sents,\n",
    "#     'num_words': sents_decade_num_words,\n",
    "#     'word_per_sent': sents_decade_num_words / sents_decade_num_sents,\n",
    "\n",
    "# })\n",
    "# sents_decade_df.to_excel('sents_decade_df.xlsx', index=False)\n",
    "# sents_decade_df\n",
    "\n",
    "df_sents_corpus_stats = get_sents_corpus_stats(df_sents_corpus)\n",
    "df_sents_corpus_stats.to_excel('sents_corpus_stats.xlsx', index=False)\n",
    "df_sents_corpus_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5b94bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5614ca5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
