{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c646d20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n"
     ]
    }
   ],
   "source": [
    "import sys; sys.path.append('..')\n",
    "from adjective_reading.parsing import *\n",
    "import networkx as nx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f85700d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "91549it [00:02, 38935.85it/s]\n"
     ]
    }
   ],
   "source": [
    "def get_keyword_sent_corpus():\n",
    "    df = pd.read_csv(os.path.join(PATH_DATA, 'keyword_sent_corpus.csv.gz'))\n",
    "    df['IN_STASH_NLP'] = [(url,sent) in STASH_NLP for url,sent in tqdm(zip(df['url'], df['context1']))]\n",
    "    return df\n",
    "\n",
    "df = get_keyword_sent_corpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27ab4c4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IN_STASH_NLP\n",
       "False    48965\n",
       "True     42584\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.IN_STASH_NLP.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01a3451f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38943"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keys = STASH_NLP.keys_l()\n",
    "# Counter(url for url,sent in keys)\n",
    "len(keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3da0592d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detokenize_tree(tree_str):\n",
    "    toks = [x.split(')')[0] for x in tree_str.split() if x.endswith(')')]\n",
    "    return ' '.join(toks).replace(\" ,\",\",\").replace(\" .\",\".\").replace(\" !\",\"!\").replace(\" ?\",\"?\")\n",
    "\n",
    "def find_lowest_common_constituent(tree, id1, id2):\n",
    "    \"\"\"\n",
    "    Find smallest constituent containing both word IDs.\n",
    "    id1, id2 are 1-based word IDs from Stanza's dependency parse.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Map 1-based IDs to 0-based leaf indices\n",
    "    idx1, idx2 = id1 - 1, id2 - 1\n",
    "    \n",
    "    # 2. Helper to assign (start, end) leaf indices to every node in the tree\n",
    "    def assign_leaf_ranges(node, next_idx=0):\n",
    "        if node.is_leaf():\n",
    "            node.start_idx = next_idx\n",
    "            node.end_idx = next_idx\n",
    "            return next_idx + 1\n",
    "        \n",
    "        node.start_idx = float('inf')\n",
    "        node.end_idx = float('-inf')\n",
    "        \n",
    "        for child in node.children:\n",
    "            next_idx = assign_leaf_ranges(child, next_idx)\n",
    "            node.start_idx = min(node.start_idx, child.start_idx)\n",
    "            node.end_idx = max(node.end_idx, child.end_idx)\n",
    "            \n",
    "        return next_idx\n",
    "\n",
    "    # Initialize ranges if they don't exist\n",
    "    if not hasattr(tree, 'start_idx'):\n",
    "        assign_leaf_ranges(tree)\n",
    "    \n",
    "    # 3. Recursive search using ranges\n",
    "    def find_smallest(node, i1, i2):\n",
    "        # Check if this node contains both indices\n",
    "        if not (node.start_idx <= i1 <= node.end_idx and \n",
    "                node.start_idx <= i2 <= node.end_idx):\n",
    "            return None\n",
    "        \n",
    "        # Check if any child contains both (to find the *lowest* common one)\n",
    "        for child in node.children:\n",
    "            result = find_smallest(child, i1, i2)\n",
    "            if result is not None:\n",
    "                return result\n",
    "        \n",
    "        # No child contains both, so this node is the LCA\n",
    "        return node\n",
    "    \n",
    "    return find_smallest(tree, idx1, idx2)\n",
    "\n",
    "def find_smallest_str(tree, id1, id2):\n",
    "    lca = find_lowest_common_constituent(tree, id1, id2)\n",
    "    # remove POS\n",
    "    \n",
    "    if lca is None:\n",
    "        return None\n",
    "    return str(lca)\n",
    "\n",
    "    \n",
    "\n",
    "def find_smallest_str_detokenized(tree, id1, id2):\n",
    "    s = find_smallest_str(tree, id1, id2)\n",
    "    return detokenize_tree(s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca10474b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b60386c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# detokenize_tree('(S (NP (NNS Reading)) (VP (VBZ is) (NP (JJ immaterial)) (, ,) (ADVP (RB distant)) (VP (VBZ is) (NP (JJ pointless)))) (. .))')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "46c4bb78",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_sentence_graph(sentence):\n",
    "    \"\"\"Create simplified syntactic graph with collapsed case markers and conjunctions\"\"\"\n",
    "    G = nx.Graph()\n",
    "    \n",
    "    # Build index\n",
    "    id2idstr = {word.id: f\"{word.id:02d}_{word.text}\" for word in sentence.words}\n",
    "    \n",
    "    # Find case markers and conjunction markers\n",
    "    case_map = {}  # {governed_noun_id: (preposition_text, prep_id)}\n",
    "    cc_map = {}    # {coordinated_word_id: (conjunction_text, cc_id)}\n",
    "    \n",
    "    for word in sentence.words:\n",
    "        if word.deprel == 'case':\n",
    "            case_map[word.head] = (word.text, word.id)\n",
    "        elif word.deprel == 'cc':\n",
    "            cc_map[word.head] = (word.text, word.id)\n",
    "    \n",
    "    # Add nodes (excluding case and cc markers)\n",
    "    for word in sentence.words:\n",
    "        if word.deprel in ['case', 'cc']:\n",
    "            continue\n",
    "        G.add_node(\n",
    "            id2idstr[word.id],\n",
    "            idx=word.id,\n",
    "            text=word.text,\n",
    "            pos=word.pos,\n",
    "            lemma=word.lemma\n",
    "        )\n",
    "\n",
    "    # Add edges with simplified relations\n",
    "    for word in sentence.words:\n",
    "        if word.deprel in ['case', 'cc'] or word.head == 0:\n",
    "            continue\n",
    "        \n",
    "        head_id = word.head\n",
    "        child_id = word.id\n",
    "        original_deprel = word.deprel\n",
    "        \n",
    "        # Default: relation is the same as the original deprel\n",
    "        edge_attrs = {\n",
    "            'deprel': original_deprel,\n",
    "            'relation': original_deprel,\n",
    "            'phrase': find_smallest_str_detokenized(sentence.constituency, word.id, head_id)\n",
    "        }\n",
    "        \n",
    "        # 1. Collapse prepositions (case) -> meta category \"prep\"\n",
    "        if child_id in case_map:\n",
    "            prep_text, _ = case_map[child_id]\n",
    "            edge_attrs['prep'] = prep_text\n",
    "            edge_attrs['relation'] = 'prep'\n",
    "            \n",
    "        # 2. Collapse conjunctions (cc) -> meta category \"conj_[word]\"\n",
    "        if child_id in cc_map:\n",
    "            cc_text, _ = cc_map[child_id]\n",
    "            edge_attrs['cc'] = cc_text\n",
    "            # Only upgrade to meta-category if the link itself is a conjunction\n",
    "            if original_deprel == 'conj':\n",
    "                edge_attrs['relation'] = f'conj_{cc_text.lower()}'\n",
    "\n",
    "        G.add_edge(\n",
    "            id2idstr[head_id],\n",
    "            id2idstr[child_id],\n",
    "            **edge_attrs\n",
    "        )\n",
    "\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb8a7ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9036e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b587ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentdoc = get_nlp_doc(\"Close or distant reading is immaterial.\")\n",
    "# sentdoc.sentences[0].words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c48f49c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "g=get_sentence_graph(sentdoc.sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4450c4ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('01_Close',\n",
       "  '04_reading',\n",
       "  {'deprel': 'amod',\n",
       "   'relation': 'amod',\n",
       "   'phrase': 'Close or distant reading'}),\n",
       " ('01_Close',\n",
       "  '03_distant',\n",
       "  {'deprel': 'conj',\n",
       "   'relation': 'conj_or',\n",
       "   'phrase': 'Close or distant',\n",
       "   'cc': 'or'}),\n",
       " ('04_reading',\n",
       "  '06_immaterial',\n",
       "  {'deprel': 'nsubj',\n",
       "   'relation': 'nsubj',\n",
       "   'phrase': 'Close or distant reading is immaterial.'}),\n",
       " ('05_is',\n",
       "  '06_immaterial',\n",
       "  {'deprel': 'cop', 'relation': 'cop', 'phrase': 'is immaterial'}),\n",
       " ('06_immaterial',\n",
       "  '07_.',\n",
       "  {'deprel': 'punct',\n",
       "   'relation': 'punct',\n",
       "   'phrase': 'Close or distant reading is immaterial.'})]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(g.edges(data=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4bb3ad33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 5)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g.order(),g.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6fdc1e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3de70453",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import random\n",
    "\n",
    "# key=random.choice(keys)\n",
    "# key\n",
    "key = ('http://www.jstor.org/stable/44016498',\n",
    " \"Miller's notes are usually very helpful and thorough but they provide the only documentation of his references and extensive further reading; unfortunately, there is no bibliography.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "461f5795",
   "metadata": {},
   "outputs": [],
   "source": [
    "import stanza\n",
    "\n",
    "docstr = STASH_NLP[key]\n",
    "doc = stanza.Document.from_serialized(docstr)\n",
    "# doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2ec7cc69",
   "metadata": {},
   "outputs": [],
   "source": [
    "docg = get_sentence_graph(doc.sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3d52e4de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('18_references',\n",
       "  '22_reading',\n",
       "  {'deprel': 'conj',\n",
       "   'relation': 'conj_and',\n",
       "   'phrase': 'his references and extensive further reading',\n",
       "   'cc': 'and'}),\n",
       " ('20_extensive',\n",
       "  '22_reading',\n",
       "  {'deprel': 'amod',\n",
       "   'relation': 'amod',\n",
       "   'phrase': 'extensive further reading'}),\n",
       " ('21_further',\n",
       "  '22_reading',\n",
       "  {'deprel': 'amod',\n",
       "   'relation': 'amod',\n",
       "   'phrase': 'extensive further reading'})]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(a,b,d) for a,b,d in docg.edges(data=True) if 'reading' in a.lower() or 'reading' in b.lower()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "77a99e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(g.nodes(data=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5a0c4f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keyword_rels(g, keyword):\n",
    "    def rename_idx(d):\n",
    "        return {\n",
    "            k if k != 'idx' else 'id': v\n",
    "            for k,v in d.items()\n",
    "        }\n",
    "    \n",
    "    out = []\n",
    "    for n,d in g.nodes(data=True):\n",
    "        if d.get('text', '').lower() == keyword:\n",
    "            neighbors = list(g.neighbors(n))\n",
    "            for neighbor in neighbors:\n",
    "                neighbor_data = g.nodes[neighbor]\n",
    "                edge_data = g.get_edge_data(n, neighbor)\n",
    "                head_data = {\n",
    "                    **{f'head_{k}': v for k,v in neighbor_data.items()}\n",
    "                }\n",
    "                this_data = {**rename_idx(d), **edge_data, **head_data, **d}\n",
    "                out.append(this_data)\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a8edcad4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 22,\n",
       "  'text': 'reading',\n",
       "  'pos': 'NOUN',\n",
       "  'lemma': 'reading',\n",
       "  'deprel': 'amod',\n",
       "  'relation': 'amod',\n",
       "  'phrase': 'extensive further reading',\n",
       "  'head_idx': 20,\n",
       "  'head_text': 'extensive',\n",
       "  'head_pos': 'ADJ',\n",
       "  'head_lemma': 'extensive',\n",
       "  'idx': 22},\n",
       " {'id': 22,\n",
       "  'text': 'reading',\n",
       "  'pos': 'NOUN',\n",
       "  'lemma': 'reading',\n",
       "  'deprel': 'amod',\n",
       "  'relation': 'amod',\n",
       "  'phrase': 'extensive further reading',\n",
       "  'head_idx': 21,\n",
       "  'head_text': 'further',\n",
       "  'head_pos': 'ADJ',\n",
       "  'head_lemma': 'further',\n",
       "  'idx': 22},\n",
       " {'id': 22,\n",
       "  'text': 'reading',\n",
       "  'pos': 'NOUN',\n",
       "  'lemma': 'reading',\n",
       "  'deprel': 'conj',\n",
       "  'relation': 'conj_and',\n",
       "  'phrase': 'his references and extensive further reading',\n",
       "  'cc': 'and',\n",
       "  'head_idx': 18,\n",
       "  'head_text': 'references',\n",
       "  'head_pos': 'NOUN',\n",
       "  'head_lemma': 'reference',\n",
       "  'idx': 22}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_keyword_rels(docg, 'reading')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "15ee40bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_all_keyword_rels(keys=None, n=None, shuffle=True    ):\n",
    "#     if keys is None:\n",
    "#         keys = STASH_NLP.keys_l()\n",
    "#     if shuffle:\n",
    "#         random.shuffle(keys)\n",
    "#     out = []\n",
    "#     for url,sent in tqdm(keys):\n",
    "#         docstr = STASH_NLP[(url,sent)]\n",
    "#         doc = stanza.Document.from_serialized(docstr)\n",
    "#         docg = stanza_to_simplified_graph(doc.sentences[0])\n",
    "#         for d in get_keyword_rels(docg, 'reading'):\n",
    "#             out.append({\n",
    "#                 'url': url,\n",
    "#                 'context': sent,\n",
    "#                 **d\n",
    "#             })\n",
    "#         if n is not None and len(out) >= n:\n",
    "#             break\n",
    "#     return out[:n]\n",
    "\n",
    "        \n",
    "\n",
    "def get_all_keyword_rels(keys=None, n=None, shuffle=True):\n",
    "    df = get_keyword_sent_corpus()\n",
    "    df_parsed = df[df.IN_STASH_NLP]\n",
    "    out = []\n",
    "    for i,row in tqdm(df_parsed.iterrows(), total=len(df_parsed)):\n",
    "        url,sent = row['url'],row['context1']\n",
    "        docstr = STASH_NLP[(url,sent)]\n",
    "        doc = stanza.Document.from_serialized(docstr)\n",
    "        docg = get_sentence_graph(doc.sentences[0])\n",
    "        for d in get_keyword_rels(docg, 'reading'):\n",
    "            out.append({\n",
    "                **row.to_dict(),\n",
    "                **d\n",
    "            })\n",
    "        if n is not None and len(out) >= n:\n",
    "            break\n",
    "    return out[:n]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5db806fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "91549it [00:01, 53118.50it/s]\n",
      "100%|██████████| 42584/42584 [01:15<00:00, 560.40it/s]\n"
     ]
    }
   ],
   "source": [
    "odf = pd.DataFrame(get_all_keyword_rels(n=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8115a664",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "url                            http://www.jstor.org/stable/461288\n",
       "page_num                                                        4\n",
       "sent_num                                                        4\n",
       "sent            We are all our lifetime __reading__ the copiou...\n",
       "context0        194 Pascal et le d6s6quilibre ment cette circu...\n",
       "context1        We are all our lifetime reading the copious se...\n",
       "context2        One moral we have already deduced, in consider...\n",
       "token_num                                                       5\n",
       "token0                                                   lifetime\n",
       "token1                                                    reading\n",
       "token2                                                        the\n",
       "lemma                                                        read\n",
       "pos                                                          VERB\n",
       "year                                                         1967\n",
       "decade                                                       1960\n",
       "period                                                  1960-1980\n",
       "IN_STASH_NLP                                                 True\n",
       "id                                                              6\n",
       "text                                                      reading\n",
       "deprel                                                        acl\n",
       "relation                                                      acl\n",
       "phrase          are all our lifetime reading the copious sense...\n",
       "head_idx                                                        5\n",
       "head_text                                                lifetime\n",
       "head_pos                                                     NOUN\n",
       "head_lemma                                               lifetime\n",
       "idx                                                             6\n",
       "prep                                                          NaN\n",
       "cc                                                            NaN\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "odf.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01dd996b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9bb8313d",
   "metadata": {},
   "outputs": [],
   "source": [
    "odf.to_csv('../data/keyword_sent_corpus_parsed.csv.gz', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "20077105",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'are all our lifetime reading the copious sense of this first of forms'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "odf.iloc[0].phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e448eb72",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
